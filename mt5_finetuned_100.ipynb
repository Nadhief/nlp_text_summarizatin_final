{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: Could not find module 'C:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python310\\Lib\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import MT5ForConditionalGeneration, MT5Tokenizer, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load dataset\n",
    "dset = load_dataset(\"SEACrowd/liputan6\", trust_remote_code=True)\n",
    "\n",
    "# Use subsets of the dataset\n",
    "train_data = dset[\"train\"].select(range(100))\n",
    "val_data = dset[\"validation\"].select(range(20))\n",
    "test_data = dset[\"test\"].select(range(20))\n",
    "\n",
    "# Load tokenizer and model\n",
    "model_name = \"google/mt5-small\"\n",
    "tokenizer = MT5Tokenizer.from_pretrained(model_name)\n",
    "model = MT5ForConditionalGeneration.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/100 [00:00<?, ? examples/s]c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3953: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 585.15 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:00<00:00, 476.05 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:00<00:00, 453.01 examples/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Preprocessing function\n",
    "def preprocess_function(examples):\n",
    "    inputs = [\"summarize: \" + doc for doc in examples[\"document\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding=True)\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples[\"summary\"], max_length=128, truncation=True, padding=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# Tokenize dataset\n",
    "tokenized_train = train_data.map(preprocess_function, batched=True, remove_columns=train_data.column_names)\n",
    "tokenized_val = val_data.map(preprocess_function, batched=True, remove_columns=val_data.column_names)\n",
    "tokenized_test = test_data.map(preprocess_function, batched=True, remove_columns=test_data.column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_3876\\1686153763.py:19: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    ")\n",
    "\n",
    "# Define the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    tokenizer=tokenizer,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/75 [00:00<?, ?it/s]Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      " 13%|â–ˆâ–Ž        | 10/75 [01:25<09:00,  8.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 41.7127, 'grad_norm': 1503.1400146484375, 'learning_rate': 1.7333333333333336e-05, 'epoch': 0.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|â–ˆâ–ˆâ–‹       | 20/75 [02:48<07:31,  8.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 40.829, 'grad_norm': 3812.9248046875, 'learning_rate': 1.4666666666666666e-05, 'epoch': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 25/75 [03:36<06:32,  7.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 22.271984100341797, 'eval_runtime': 8.7052, 'eval_samples_per_second': 2.297, 'eval_steps_per_second': 0.574, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 30/75 [04:41<08:23, 11.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 40.7342, 'grad_norm': 2222.057373046875, 'learning_rate': 1.2e-05, 'epoch': 1.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 40/75 [05:59<04:41,  8.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 38.6041, 'grad_norm': 1497.711669921875, 'learning_rate': 9.333333333333334e-06, 'epoch': 1.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 50/75 [07:22<03:35,  8.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 37.5655, 'grad_norm': 2438.293701171875, 'learning_rate': 6.666666666666667e-06, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 50/75 [07:31<03:35,  8.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 20.08449935913086, 'eval_runtime': 9.4463, 'eval_samples_per_second': 2.117, 'eval_steps_per_second': 0.529, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 60/75 [09:20<02:21,  9.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 37.8014, 'grad_norm': 3073.8251953125, 'learning_rate': 4.000000000000001e-06, 'epoch': 2.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 70/75 [10:52<00:45,  9.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 37.9038, 'grad_norm': 1303.491943359375, 'learning_rate': 1.3333333333333334e-06, 'epoch': 2.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 75/75 [12:00<00:00,  9.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 19.302310943603516, 'eval_runtime': 14.6585, 'eval_samples_per_second': 1.364, 'eval_steps_per_second': 0.341, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight'].\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 75/75 [12:13<00:00,  9.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 733.8765, 'train_samples_per_second': 0.409, 'train_steps_per_second': 0.102, 'train_loss': 39.19485677083333, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=75, training_loss=39.19485677083333, metrics={'train_runtime': 733.8765, 'train_samples_per_second': 0.409, 'train_steps_per_second': 0.102, 'total_flos': 158624907264000.0, 'train_loss': 39.19485677083333, 'epoch': 3.0})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average BLEU Score: 9.264942934643022e-158\n",
      "Article: Liputan6 . com , Bangka : Kapal patroli Angkatan Laut Republik Indonesia , Belinyu , baru-baru ini , menangkap tiga kapal nelayan berbendera Thailand , yakni KM Binatama , KM Sumber Jaya II , dan KM Mataram di Perairan Belitung Utara . Ketiga kapal itu ditangkap karena melanggar zona ekonomi ekslusif Indonesia . Saat ini , kapal-kapal itu diamankan di Pos Lanal Pelabuhan Pangkalan Balam , Bangka-Belitung . Menurut Komandan Pangkalan TNI AL Bangka Letnan Kolonel Laut Fredy Egam , selain menangkap tiga kapal , ALRI juga memeriksa 43 anak buah kapal . Mereka disergap saat sedang mengangkat jaring pukat harimau di Perairan Belitung Utara . Dari jumlah itu , hanya enam orang yang dijadikan tersangka , yakni tiga nahkoda dan tiga kepala kamar mesin kapal . Sedangkan ABK yang lain akan dideportasi ke negara asalnya . Meski berhasil menahan enam tersangka , TNI AL gagal mengamankan ikan tangkapan nelayan Thailand tersebut . Sebab , sebelum patroli datang , mereka telah memindahkan puluhan ton ikan hasil jaringan ke kapal induk . ( ULF/Ajmal Rokian dan Yanuar Ichrom ) ....\n",
      "Reference Summary: Meski memiliki izin resmi , TNI AL tetap menangkap tiga kapan nelayan berbendera Thailand . Pasalnya , ketiga kapal itu melanggar zona ekonomi ekslusif dan menjaring ikan dengan pukat harimau .\n",
      "Predicted Summary: <extra_id_0> Bangka :\n",
      "BLEU Score: 0\n",
      "\n",
      "Article: Liputan6 . com , Bandar Lampung : Sebanyak 51 anak di bawah umur lima tahun terserang busung lapar atau marasmus karena kekurangan gizi di Kota Madya Bandar Lampung . Lima di antaranya tewas . Data tersebut diungkapkan Kepala Dinas Kesehatan Kota Bandar Lampung M . Sudarman , baru-baru ini . Menurut Sudarman , Dinas Kesehatan Bandar Lampung mencatat sekitar 51 anak terserang busung lapar yang tersebar di beberapa kecamatan , selama periode 1999 sampai 2001 . Kebanyakan anak penderita busung tersebut berasal dari keluarga yang hidup di bawah garis kemiskinan . Selain kekurangan gizi , komplikasi radang paru-paru juga menjadi satu faktor penyebab kematian anak penderita busung lapar tersebut . Data Dinas Kesehatan menunjukkan pada 1999 , ditemukan 41 anak terserang penyakit busung lapar . Sebagian besar penderita berdomisili di kampung miskin Umbul Kunci . Jumlah penderita busung lapar menurun pada 2000 , yakni hanya sembilan anak . Sedangkan September 2001 , seorang anak meninggal karena marasmus . Sudarman menegaskan , untuk menekan jumlah korban marasmus , anak-anak dan balita diberi makanan tambahan ke sentra-sentra rawan busung lapar seperti di Desa Umbul Unci . Tetapi , berdasarkan keterangan masyarakat Umbul Kunci , program makanan tambahan ini hanya berjalan setahun yakni pada 1999 . Warga setempat mengaku tak pernah lagi menerima makanan tambahan bagi anak-anak kurang gizi sejak dua tahun terakhir . ( TNA/Bisri Merduani ) ....\n",
      "Reference Summary: Sebanyak 51 anak di bawah usia lima tahun di Kota Madya Bandar Lampung , menderita busung lapar karena kekurangan gizi . Lima di antaranya meninggal dunia .\n",
      "Predicted Summary: <extra_id_0> Marasmus\n",
      "BLEU Score: 0\n",
      "\n",
      "Article: Liputan6 . com , Jakarta : Polemik seputar pelaksanaan Sidang Istimewa MPR , 1 Agustus mendatang , terus bergulir . Setelah sekian kali , Presiden Abdurrahman Wahid kembali mengeluarkan ancaman akan memberlakukan status keadaan bahaya dan darurat sipil . Langkah itu bakal dilakukan jika SI MPR tetap meminta pertanggungjawaban dirinya . Penegasan tersebut diungkapkan Presiden Wahid dalam dialog usai salat Jumat di Mesjid Al Munawarah , Ciganjur , Jakarta Selatan , Jumat ( 06/7 ) . Bagi Gus Dur , pada dasarnya dia tidak mempermasalahkan pelaksanaan SI MPR . Asalkan , agenda yang dibahas tidak menyangkut pertanggungjawaban dan kinerja pemerintahannya . \" Sebab , masih ada agenda lain yang bisa dibahas , demi kemajuan negara ini , \" kata Gus Dur , serius . Pernyataan Gus Dur kontan ditanggap Wakil Rakyat . Menurut anggota Fraksi Partai Bulan Bintang DPR Hamdan Zoelva , pemberlakuan keadaan darurat sipil justru akan mempermudah aparat keamanan untuk mengantisipasi . Jadi , siapa pun yang berniat mengganggu jalannya SI MPR , bisa diantisipasi sejak awal . Meski begitu , bagi Hamdan , saat ditemui usai dialog soal voting dan money politics Jumat siang tadi , keadaan darurat sipil tadi tak boleh salah arah . Misalnya , bila diberlakukan untuk melarang anggota dewan bersidang . \" Itu menorehkan sejarah hitam sistem ketatanegaraan kita , \" kata Hamdan . Hamdan juga menambahkan , bila darurat sipil hanya semata-mata untuk menghambat jalannya SI MPR , nilai tambah buat Gus Dur . Bahkan , tidak menutup kemungkinan malah memicu kebulatan tekad keputusan sidang Wakil Rakyat untuk menolak pertanggungjawaban Presiden Wahid . ( BMI/Tim Liputan 6 SCTV ) ....\n",
      "Reference Summary: Presiden Abdurrahman Wahid kembali mengeluarkan ancaman akan memberlakukan status keadaan darurat sipil . Terutama , jika Sidang Istimewa MPR 1 Agustus mendatang tetap meminta pertanggungjawaban dirinya .\n",
      "Predicted Summary: <extra_id_0> .\n",
      "BLEU Score: 4.100152922267089e-234\n",
      "\n",
      "Article: Liputan6 . com , Ambon : Bahan bakar minyak jenis solar dan premium , selama sepekan terakhir , mulai langka di Kota Ambon . Akibatnya , sebagian besar kendaraan umum di Ambon memilih berhenti beroperasi karena sulit mendapatkan bahan bakar . Selain itu , kelangkaan tersebut juga memicu kenaikan harga premium di tingkat pengecer : mencapai Rp 2 ribu per liter . Padahal , biasanya harga premium hanya Rp 1 . 700 per liter . Demikian hasil pantauan SCTV di Ambon , baru-baru ini . Beberapa pedagang minyak eceran mengaku , kelangkaan terjadi karena pasokan dari Depot Pertamina Wayame berkurang . Selain itu , mereka juga menduga kelangkaan terjadi lantaran adanya ulah oknum yang menyuplai minyak ke sejumlah kapal di Pelabuhan Maluku . Menanggapi itu , Kapala Cabang Pertamina Unit Pemasaran dan Perbekalan Dalam Negeri VIII Ambon Subaedi memastikan , kelangkaan terjadi karena adanya kebijakan Pertamina Pusat yang mengurangi jatah pasokan . Subaedi memperkirakan , kelangkaan BBM di Ambon akan terus berlangsung hingga bulan depan . Tapi , ia menolak jika kelangkaan terjadi akibat adanya kerja sama kotor antara Pertamina dan para spekulan . ( ICH/Sahlan Heluth ) ....\n",
      "Reference Summary: Sepekan terakhir , bahan bakar minyak jenis solar dan premium di Kota Ambon , mulai langka . Kelangkaan terjadi karena pasokan BBM dari Depot Pertamina Wayame berkurang .\n",
      "Predicted Summary: <extra_id_0> .\n",
      "BLEU Score: 3.193202306574118e-234\n",
      "\n",
      "Article: Liputan6 . com , Jakarta : Seluruh perubahan pasal Undang-Undang Nomor 23 tahun 1999 tentang Bank Indonesia telah disetujui oleh pemerintah dan DPR . Namun masih ada satu polemik soal pasal 75 . Inti pasal itu adalah soal memberhentikan seluruh anggota Dewan Gubernur BI , setelah UU Amendemen BI berlaku efektif . Bagi BI sendiri , posisinya harus tetap independen . Pernyataan itu ditegaskan Gubernur BI Sjahril Sabirin , baru-baru ini , di Jakarta . Menurut Sjahril , persoalan amendemen sudah menjadi kewenangan pemerintah dan DPR . Sebab kedua lembaga tersebut telah mendapat masukan dari tim panel Dana Moneter Internasional ( IMF ) . \" IMF sudah menganjurkan pembentukan tim panel , \" kata Sjahril . Lantas , tambah dia , tim panel itu juga sudah mengeluarkan hasil keputusan . \" Sekarang , tergantung pemerintah dan DPR untuk menerapkannya , \" kata Sjahril , serius . Pembahasan pasal 75 sendiri masih berlangsung di Panitia Kerja DPR . Panja ini mengusulkan pilihan supaya pemerintah mempercepat pembentukan Dewan Supervisi BI . Lantas , panja juga menawarkan kepada pemerintah untuk membuat laporan mengenai penyimpangan di tubuh BI . Berdasarkan laporan ini , DPR mesti menggelar paripurna untuk menentukan perlu tidaknya mengganti Gubernur BI . ( COK/Olivia Rosalia dan Bambang Triono ) ....\n",
      "Reference Summary: Bank Indonesia menyerahkan sepenuhnya amendemen UU BI tentang pergantian dewan gubernur kepada pemerintah dan DPR . Diperkirakan , polemik soal pasal 75 sulit untuk bisa mencapai kata sepakat .\n",
      "Predicted Summary: <extra_id_0> .\n",
      "BLEU Score: 2.4868684568653395e-234\n",
      "\n",
      "Article: Liputan6 . com , Jakarta : Mantan Presiden B . J . Habibie , Rabu ( 27/11 ) , kembali tak memenuhi panggilan Kejaksaan Agung . Sedianya , Habibie akan diperiksa sebagai saksi dalam kasus penggunaan dana nonbujeter Badan Urusan Logistik senilai Rp 54 , 6 miliar yang diduga melibatkan mantan Menteri Sekretaris Negara Akbar Tandjung . Bila tak ada aral melintang , pengacara Habibie , Yan Juanda , akan tiba di Kejagung sekitar pukul 12 . 00 WIB . Yan Juanda akan menjelaskan bahwa Habibie tak bisa memenuhi panggilan Kejagung untuk kedua kalinya karena masih berada di Jerman untuk menemani istrinya yang sakit . Menurut Kepala Pusat Penerangan dan Hukum Kejagung Moeljohardjo , sebenarnya kehadiran Habibie sangat diperlukan untuk mengungkap kasus tersebut . Sebab , Kejagung benar-benar serius untuk menyelesaikan Kasus Bulog dengan segera . Apabila , Habibie tak bisa datang ke Indonesia , bukan tak mungkin tim penyidik Kejagung akan ke Jerman untuk menemui Habibie . ( ULF/Edi Priono dan Irfan Efendi ) ....\n",
      "Reference Summary: Kejaksaan Agung akan mengirim tim penyidik ke Jerman untuk menemui mantan Presiden B . J . Habibie . Mantan Presiden ketiga itu kembali tak memenuhi panggilan Kejagung .\n",
      "Predicted Summary: <extra_id_0> .\n",
      "BLEU Score: 3.193202306574118e-234\n",
      "\n",
      "Article: Liputan6 . com , Jakarta : Unjuk rasa anti-Amerika Serikat yang kini tengah memanas , ternyata telah menyusahkan sejumlah karyawan yang bekerja di sekitar Kedutaan Besar AS , Jalan Medan Merdeka Selatan , Jakarta Pusat . Pasalnya , jalan tersebut kerap ditutup untuk arus lalu lintas . Akibatnya , mereka harus rela berjalan kaki cukup jauh dari tempat bekerja . Untuk itu , mereka berharap agar unjuk rasa sejumlah elemen masyarakat yang menentang Amerika dapat dilakukan dengan tertib . \" Supaya transportasi tak terganggu , \" kata Ade , seorang karyawan , di Jakarta , baru-baru ini . Menurut Ade , kendaraan miliknya terpaksa diparkir di lokasi yang cukup jauh dari kantornya . Selain itu , meski terbiasa melihat unjuk rasa , sekarang ini , mereka suka was-was dan khawatir terjadi bentrokan lantaran yang berdemo terlihat lebih brutal . ( DEN/Apriliana dan Agus Ginanjar ) ....\n",
      "Reference Summary: Karyawan yang berkantor di sekitar Kedubes AS terpaksa berjalan kaki jika terjadi aksi anti-Amerika . Unjuk rasa diharapkan berlangsung lebih tertib .\n",
      "Predicted Summary: <extra_id_0> Amerika Serikat\n",
      "BLEU Score: 0\n",
      "\n",
      "Article: Liputan6 . com , Jakarta : Menteri Pendidikan Nasional Yahya Muhaimin meminta unsur dan instansi yang terlibat dalam penerimaan siswa baru tak mempersulit calon siswa . Permintaan Mendiknas tersebut disampaikan seusai melantik tujuh rektor perguruan tinggi negeri di Jakarta , Jumat ( 13/7 ) . Yahya mengakui ada beberapa pelanggaran dalam peneriman calon siswa baru tahun ini . Yahya mencontohkan ada orang tua siswa yang terpaksa menyogok panitia penerima siswa agar anaknya masuk ke sekolah favorit . Selain itu beredar juga katabelece di beberapa sekolah . Karena itu , Yahya berjanji akan menindak penyelewengan . Sementara itu , pelantikan rektor sempat tertunda . Pasalnya , Pembantu Rektor III Universitas Cendrawasih , Jayapura , Irianjaya , Robert Pilipus Lalenoh jatuh pingsan . Akhirnya , Pilipus meninggal dunia di Rumah Sakit Angkatan Laut Mintohardjo Jakarta . ( YYT/Tri Ambarwatie dan Rafael Setyo ) ....\n",
      "Reference Summary: Sejumlah penyelewengan dalam penerimaan calon siswa baru dapat diidentifikasi . Karena itu , dalam waktu dekat Mendiknas Yahya Muhaimin berjanji membentuk sebuah tim untuk menindak pelaku penyelewengan .\n",
      "Predicted Summary: <extra_id_0> .\n",
      "BLEU Score: 3.193202306574118e-234\n",
      "\n",
      "Article: Liputan6 . com , Jakarta : Seorang bapak dan anak ditemukan tewas dengan kondisi mengenaskan di kediamannya di Duri Selatan RT 4 , RW 5 Nomor 2 , Jakarta Barat , Senin ( 21/5 ) sekitar pukul 11 . 00 . Sejauh ini , polisi belum mengetahui penyebab kematian kedua korban . Yang pasti , polisi menemukan pisau bernoda darah dan luka di sekujur tubuh kedua korban . Demikian diungkapkan Kepala Kepolisian Sektor Tambora Ajun Komisaris Polisi Merdisyam , di Jakarta , tadi malam . Menurut Merdisyam , Hendrikus , 41 tahun dan anaknya , Michael , 4 tahun tewas sangat mengenaskan dengan luka di sekujur tubuh . Ia menambahkan sejumlah barang bukti di antaranya pisau bernoda darah ditemukan di tempat kejadian . Menurut Merdisyam , mayat Hendrikus , 41 tahun dan anaknya , Michael 4 tahun pertama kali ditemukan istrinya , Afa . Kala itu , Afa yang baru tiba dari rumah saudaranya melihat pintu rumah depan tak terkunci . Sedangkan pintu pagar rumah dalam keadaan terkunci . Ketika masuk rumahnya , ibu dua anak itu mendapati suami dan anaknya tewas dalam kondisi mengenaskan . Dari keterangan yang dihimpun SCTV menyebutkan , sejumlah warga yang berada di sekitar lokasi tak sedikit pun mendengar kegaduhan di rumah korban . Warga juga mengaku , tak ada tamu yang datang ke rumah korban sejak petang hari . Untuk mengungkap kasus ini , polisi menurunkan anjing pelacak , memeriksa istri korban , dan sejumlah saksi . Namun hingga kini , polisi belum dapat memastikan : apakah kedua korban terbunuh atau bunuh diri ? ( AWD/Christiyanto dan Agus Kusnohadi ) ....\n",
      "Reference Summary: Hendrikus , 41 tahun dan anaknya , Michael , 4 tahun ditemukan tewas mengenaskan di rumahnya , di Duri Selatan RT 4 , RW 5 Nomor 2 , Jakarta Barat . Mayat Hendrikus dan anaknya ditemukan istrinya , Afa .\n",
      "Predicted Summary: <extra_id_0> .\n",
      "BLEU Score: 1.5898018154982647e-235\n",
      "\n",
      "Article: Liputan6 . com , Jakarta : Sekretaris Fraksi Reformasi ( FR ) DPR Alvin Lie Ling Pao mengadukan rekan sefraksinya , Syamsul Balda ke Kepolisian Daerah Metro Jaya , Selasa ( 11/9 ) . Alvin menuduh Syamsul mencemarkan nama baiknya sebagai anggota DPR . Alvin menjelaskan , perkara itu bermula dari pernyataan Syamsul di sejumlah media massa soal posisi sekretaris FR yang dipegangnya . Kala itu , menurut Alvin , Syamsul mengatakan Alvin hanya mencapai posisi terakhir saat pemilihan ketua FR , sehingga tak layak menduduki posisi sekretaris fraksi . Pendapat Syamsul itulah , menurut Alvin , yang membuatnya geram , sehingga ia melaporkan pekara itu ke polisi . Sebaliknya , Syamsul menyayangkan tindakan Wakil Sekretaris Jenderal Partai Amanat Nasional itu . Apalagi , ia mengaku , Alvin tak pernah secara langsung menanyakan soal itu kepadanya . Karena itu , ia berjanji akan membeberkan duduk perkara tuduhan tersebut bila dipanggil polisi . Keinginan Syamsul tampaknya bakal menjadi kenyataan . Sebab , selain akan menyelidiki pengaduan tersebut , polisi juga berniat mengirim surat permohonan pemeriksaan terhadap Syamsul kepada Presiden . ( AWD/Tim Liputan 6 SCTV ) ....\n",
      "Reference Summary: Sekretaris Fraksi Reformasi Alvin Lie mengadukan rekan sefraksinya , Syamsul Balda ke Polisi dengan tuduhan pencemaran nama baik . Syamsul menganggap Alvin berlebihan .\n",
      "Predicted Summary: <extra_id_0> .\n",
      "BLEU Score: 8.680023804553935e-234\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "# Evaluate BLEU score\n",
    "def compute_bleu(data):\n",
    "    references = [[word_tokenize(summary)] for summary in data[\"summary\"]]\n",
    "    predictions = []\n",
    "    for article in data[\"document\"]:\n",
    "        inputs = tokenizer(\"summarize: \" + article, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "        outputs = model.generate(**inputs, max_length=128, num_beams=4, early_stopping=True)\n",
    "        pred_summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        predictions.append(word_tokenize(pred_summary))\n",
    "    return corpus_bleu(references, predictions)\n",
    "\n",
    "average_bleu = compute_bleu(test_data)\n",
    "print(f\"Average BLEU Score: {average_bleu}\")\n",
    "\n",
    "# Predict on test data and print BLEU score for each sample\n",
    "for i in range(10):  # Predict and display summaries for first 10 examples\n",
    "    article = test_data[i][\"document\"]\n",
    "    reference_summary = test_data[i][\"summary\"]\n",
    "\n",
    "    # Generate prediction\n",
    "    inputs = tokenizer(\"summarize: \" + article, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    outputs = model.generate(**inputs, max_length=128, num_beams=4, early_stopping=True)\n",
    "    pred_summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Tokenize references and prediction\n",
    "    reference_tokens = word_tokenize(reference_summary)\n",
    "    prediction_tokens = word_tokenize(pred_summary)\n",
    "\n",
    "    # Compute BLEU score for the sample\n",
    "    sample_bleu = sentence_bleu([reference_tokens], prediction_tokens)\n",
    "\n",
    "    print(f\"Article: {article}...\")\n",
    "    print(f\"Reference Summary: {reference_summary}\")\n",
    "    print(f\"Predicted Summary: {pred_summary}\")\n",
    "    print(f\"BLEU Score: {sample_bleu}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
